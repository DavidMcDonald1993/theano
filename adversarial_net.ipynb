{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GT 650M (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist, cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data params\n",
    "data_mean = 4\n",
    "data_stddev = 1.25\n",
    "\n",
    "# Model params\n",
    "g_input_size = 100     # Random noise dimension coming into generator, per output vector\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 100    # size of generated output vector\n",
    "\n",
    "d_input_size = 100  # Minibatch size - cardinality of distributions\n",
    "d_hidden_size = 50   # Discriminator complexity\n",
    "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
    "\n",
    "minibatch_size = d_input_size\n",
    "\n",
    "d_learning_rate = 2e-4  # 2e-4\n",
    "g_learning_rate = 2e-4\n",
    "optim_betas = (0.9, 0.999)\n",
    "num_epochs = 30000\n",
    "print_interval = 200\n",
    "d_steps = 5  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "g_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_distribution_sampler(mu, sigma):\n",
    "#     return lambda n : K.random_normal(mean=mu, std=sigma, shape=(1, n))\n",
    "    return lambda m, n : np.random.normal(mu, sigma, size=(m, n)).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generator_input_sampler():\n",
    "#     return lambda m, n: K.random_uniform(shape=(m, n))\n",
    "    return lambda m, n: np.random.rand(m, n).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#samplers for data distribution\n",
    "gi_sampler = get_generator_input_sampler()\n",
    "d_sampler = get_distribution_sampler(data_mean, data_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(hidden_size, input_shape = (input_size,), activation = \"elu\"))\n",
    "        self.model.add(Dense(hidden_size, activation = \"elu\"))\n",
    "#         self.model.add(Dense(hidden_size, activation = \"elu\"))\n",
    "        self.model.add(Dense(output_size, activation = \"elu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(hidden_size, input_shape = (input_size,), activation = \"elu\"))\n",
    "        self.model.add(Dense(hidden_size, activation = \"elu\"))\n",
    "        self.model.add(Dense(output_size, activation = \"sigmoid\"))\n",
    "    \n",
    "    def compileModel(**kwargs):\n",
    "        self.model.compile(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GeneratorAndDiscriminator:\n",
    "    \n",
    "    def __init__(self, generator, discriminator):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(generator)\n",
    "        \n",
    "        discriminator.trainable = False\n",
    "        self.model.add(discriminator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_progress(epoch, epochs, start_time):\n",
    "    \n",
    "    bar_length = 80\n",
    "    \n",
    "    progress_bar = \"[\" + \"=\" * int(bar_length * epoch / epochs) + \">\" + \"-\" * int(bar_length * (epochs - epoch) / epochs) + \"]\"\n",
    "    \n",
    "    secs = np.ceil((time() - start_time) * epochs / epoch)\n",
    "    hours = np.floor(secs / 3600)\n",
    "    secs -= hours * 3600\n",
    "    mins = np.floor(secs / 60)\n",
    "    secs -= mins * 60\n",
    "    \n",
    "    sys.stdout.write(\"\\r\" + progress_bar + \" Epoch {}/{} ETA:{}h {}m {}s\".format(epoch, epochs, hours, mins, secs))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##dicriminator\n",
    "# dis_in = Input(shape=(d_input_size,))\n",
    "# dis_1 = Dense(d_hidden_size, activation = \"elu\")(dis_in)\n",
    "# dis_2 = Dense(d_hidden_size, activation = \"elu\")(dis_1)\n",
    "# dis_3 = Dense(d_output_size, activation = \"sigmoid\")(dis_2)\n",
    "# D = Model(dis_in, dis_3)\n",
    "\n",
    "D = Discriminator(d_input_size, d_hidden_size, d_output_size)\n",
    "\n",
    "#compile D\n",
    "D.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##generator\n",
    "# gen_in = Input(shape=(g_input_size,))\n",
    "# gen_1 = Dense(g_hidden_size , activation = \"elu\")(gen_in)\n",
    "# gen_2 = Dense(g_hidden_size , activation = \"elu\")(gen_1)\n",
    "# gen_3 = Dense(g_output_size , activation = \"sigmoid\")(gen_2)\n",
    "# G = Model(gen_in, gen_3)\n",
    "\n",
    "G = Generator(g_input_size, g_hidden_size, g_output_size)\n",
    "\n",
    "#compile G\n",
    "G.model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##generator and discriminator\n",
    "\n",
    "GD = GeneratorAndDiscriminator(G.model, D.model)\n",
    "GD.model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=>------------------------------------------------------------------------------] Epoch 422/30000 ETA:1.0h 26.0m 40.0ss"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "main loop\n",
    "'''\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for d_step in range(d_steps):\n",
    "        \n",
    "        #train D on real data\n",
    "        d_real_data = d_sampler(100, d_input_size)\n",
    "        d_real_targets = np.ones(100)\n",
    "        \n",
    "        #train D on fake data\n",
    "        d_gen_data = gi_sampler(100, g_input_size)\n",
    "        d_fake_data = G.model.predict(d_gen_data)\n",
    "        d_fake_targets = np.zeros(100)\n",
    "        \n",
    "        #train model\n",
    "        d_data = np.append(d_real_data, d_fake_data, axis=0)\n",
    "        d_targets = np.append(d_real_targets, d_fake_targets)\n",
    "\n",
    "        #fit discriminator\n",
    "        D.model.trainable = True\n",
    "        D.model.fit(d_data, d_targets, shuffle=True, nb_epoch=10,\n",
    "              batch_size=minibatch_size, validation_split=0.0, verbose=0)\n",
    "    \n",
    "#     print \"Completed training of D for epoch {}\".format(epoch)\n",
    "        \n",
    "    for g_step in range(g_steps):\n",
    "        \n",
    "        #generate data from noise\n",
    "        gen_input = gi_sampler(100, g_input_size)\n",
    "        \n",
    "        #target\n",
    "        target = np.ones(100)\n",
    "        \n",
    "        #fit generator\n",
    "        D.model.trainable = False\n",
    "        GD.model.fit(gen_input, target, shuffle=True, nb_epoch=10,\n",
    "              batch_size=minibatch_size, validation_split=0.0, verbose=0)\n",
    "        \n",
    "        \n",
    "#     print \"Completed training of G for epoch {}\".format(epoch)\n",
    "    \n",
    "#     print \"Epoch {} complete\".format(epoch)\n",
    "    print_progress(epoch, num_epochs, start_time)\n",
    "print \"\\nDONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_input = gi_sampler(1000, g_input_size)\n",
    "forgery = G.model.predict(gen_input)\n",
    "# print D.model.predict(forgery)\n",
    "print np.mean(forgery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_real_data = d_sampler(100, d_input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.97750616,  5.25092888,  4.59791613, ...,  4.76979065,\n",
       "         4.08094931,  2.61142635],\n",
       "       [ 4.0486145 ,  4.50510311,  4.73263216, ...,  4.83788395,\n",
       "         4.6385498 ,  3.18401814],\n",
       "       [ 4.82702923,  6.47186995,  3.89000297, ...,  3.54565644,\n",
       "         5.12652969,  6.09436893],\n",
       "       ..., \n",
       "       [ 2.72764564,  4.03013802,  1.99318993, ...,  3.19162607,\n",
       "         4.72337151,  2.75904036],\n",
       "       [ 5.90803957,  1.71235585,  3.32587624, ...,  4.61864996,\n",
       "         5.31416559,  5.39929628],\n",
       "       [ 3.58024287,  3.90217638,  2.14244676, ...,  4.64943457,\n",
       "         0.63970524,  1.97230172]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
