{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GT 650M (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from time import time\n",
    "import sys\n",
    "import curses\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Dropout, Activation, Flatten, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D, Conv2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist, cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        self.model = Sequential()\n",
    "#         self.model.add(Dense(hidden_size, input_shape = (input_size,), activation = \"elu\"))\n",
    "        self.model.add(Dense(64 * 7 * 7, input_shape=(input_size,), activation = \"tanh\"))\n",
    "        self.model.add(Reshape((64, 7, 7)))\n",
    "        self.model.add(Conv2D(filters=32, kernel_size=(5, 5), padding=\"same\", use_bias=True,\n",
    "                      activation=\"tanh\", data_format=\"channels_first\"))\n",
    "        self.model.add(UpSampling2D(size=(2, 2)))\n",
    "        self.model.add(Conv2D(filters=16, kernel_size=(5, 5), padding=\"same\", use_bias=True,\n",
    "                      activation=\"tanh\", data_format=\"channels_first\"))\n",
    "        self.model.add(UpSampling2D(size=(2, 2)))\n",
    "        self.model.add(Conv2D(filters=1, kernel_size=(3, 3), padding=\"same\", use_bias=True,\n",
    "                      activation=\"tanh\", data_format=\"channels_first\"))\n",
    "        \n",
    "    def compileModel(self, **kwargs):\n",
    "        self.model.compile(**kwargs)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "        \n",
    "    def train_on_batch(self, **kwargs):\n",
    "        return self.model.train_on_batch(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(filters=32, kernel_size=5, padding=\"same\", use_bias=True,\n",
    "                                     input_shape=(1, img_rows, img_cols), activation=\"tanh\", data_format=\"channels_first\"))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Conv2D(filters=16, kernel_size=5, padding=\"same\", use_bias=True,\n",
    "                              activation=\"tanh\", data_format=\"channels_first\"))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(output_size, activation = \"sigmoid\"))\n",
    "    \n",
    "    def compileModel(self, **kwargs):\n",
    "        self.model.compile(**kwargs)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "        \n",
    "    def train_on_batch(self, X, Y):\n",
    "        return self.model.train_on_batch(X, Y)\n",
    "    \n",
    "    def trainable(self, trainable):\n",
    "        self.model.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GeneratorAndDiscriminator:\n",
    "    \n",
    "    def __init__(self, generator, discriminator):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(generator)\n",
    "        self.model.add(discriminator)\n",
    "        \n",
    "    def compileModel(self, **kwargs):\n",
    "        self.model.compile(**kwargs)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "        \n",
    "    def train_on_batch(self, X, Y):\n",
    "        return self.model.train_on_batch(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_progress(stdscr, epoch, epochs, minibatch, minibatches, start_time, g_loss, d_loss):\n",
    "    \n",
    "    bar_length = 50\n",
    "    \n",
    "    progress_bar = \"[\" + \"=\" * int(bar_length * epoch / epochs) + \\\n",
    "    \">\" + \"-\" * int(bar_length * (epochs - epoch) / epochs) + \"]\"\n",
    "    \n",
    "    time_taken = (time() - start_time)\n",
    "    \n",
    "    minibatches_complete = epoch * minibatches + minibatch\n",
    "    total_minibatches = epochs * minibatches\n",
    "    \n",
    "    secs = np.ceil(time_taken * total_minibatches / minibatches_complete - time_taken)\n",
    "    hours = np.floor(secs / 3600)\n",
    "    secs -= hours * 3600\n",
    "    mins = np.floor(secs / 60)\n",
    "    secs -= mins * 60\n",
    "    \n",
    "    stdscr.addstr(0, 0 , \"Epoch: {}/{} minibatch: {}/{} \".format(epoch, epochs, minibatch, minibatches) + \n",
    "                  progress_bar + \n",
    "                  \" {}% ETA: {}h {}m {}s\".format(int(epoch * 100 / epochs), int(hours), int(mins), int(secs)))\n",
    "    stdscr.addstr(1, 0, \"g_loss: {} \".format(g_loss[0]) + \"g_acc: {} \".format(g_loss[1]) + \n",
    "                     \"d_loss: {} \".format(d_loss[0]) + \"d_acc: {}\".format(d_loss[1]))\n",
    "    stdscr.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    stdscr.addstr(0, 0 , \"Epoch: {}/{}\".format(i, 100))\n",
    "    stdscr.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model params\n",
    "g_input_size = 100    # Random noise dimension coming into generator, per output vector\n",
    "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
    "\n",
    "# Data params\n",
    "nb_classes = 10\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##load mnist data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "##reshape data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#centre data on zero\n",
    "x_train = (x_train - 255 / 2) / (255 / 2)\n",
    "x_test = (x_test - 255 / 2) / (255 / 2)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "num_epochs = 10\n",
    "d_steps = 1 # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "g_steps = 1\n",
    "\n",
    "#parameter settings\n",
    "minibatch_size = 128\n",
    "\n",
    "num_patterns = len(x_train)\n",
    "num_minibatches = int(num_patterns / minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#construct generator\n",
    "G = Generator(g_input_size)\n",
    "\n",
    "#compile G\n",
    "G.compileModel(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#construct discriminator\n",
    "D = Discriminator( d_output_size)\n",
    "\n",
    "#compile D\n",
    "D.compileModel(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "##generator and discriminator\n",
    "GD = GeneratorAndDiscriminator(G.model, D.model)\n",
    "\n",
    "#compile GD\n",
    "GD.compileModel(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-af5364d435b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#generate fake data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0md_gen_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0md_fake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_gen_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m#         print d_fake_data.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0md_fake_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d4f422fb1006>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1572\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1200\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "'''\n",
    "main loop\n",
    "'''\n",
    "\n",
    "stdscr = curses.initscr()\n",
    "curses.noecho()\n",
    "curses.cbreak()\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "    for minibatch in range(num_minibatches):\n",
    "        \n",
    "        '''\n",
    "        train D\n",
    "        '''\n",
    "\n",
    "        #get real data\n",
    "        d_real_data = x_train[minibatch * minibatch_size : (minibatch + 1) * minibatch_size]\n",
    "#         print d_real_data.shape\n",
    "        d_real_targets = np.ones(minibatch_size)\n",
    "\n",
    "        #generate fake data\n",
    "        d_gen_input = np.random.uniform(-1, 1, size=(minibatch_size, g_input_size))\n",
    "        d_fake_data = G.predict(d_gen_input)\n",
    "#         print d_fake_data.shape\n",
    "        d_fake_targets = np.zeros(minibatch_size)\n",
    "\n",
    "        #combine real and fake data\n",
    "        d_data = np.append(d_real_data, d_fake_data, axis=0)\n",
    "        d_targets = np.append(d_real_targets, d_fake_targets)\n",
    "\n",
    "        #fit discriminator\n",
    "        D.trainable(True)\n",
    "#         d_loss = D.train_on_batch(d_data, d_targets, shuffle=True, epochs=epochs,\n",
    "#               batch_size=1, validation_split=0.0, verbose=0)\n",
    "        d_loss = D.train_on_batch(d_data, d_targets)\n",
    "        \n",
    "        '''\n",
    "        train G\n",
    "        '''\n",
    "        \n",
    "        #generate data from noise\n",
    "        g_gen_input = np.random.uniform(-1, 1, size=(minibatch_size, g_input_size))\n",
    "        \n",
    "        #target\n",
    "        g_targets = np.ones(minibatch_size)\n",
    "        \n",
    "        #fit generator\n",
    "        D.trainable(False)\n",
    "#         g_loss = GD.train_on_batch(gen_input, target, shuffle=True, epochs=epochs,\n",
    "#               batch_size=1, validation_split=0.0, verbose=0)\n",
    "        g_loss = GD.train_on_batch(g_gen_input, g_targets)\n",
    "        \n",
    "        print_progress(stdscr, epoch, num_epochs, minibatch, num_minibatches, start_time, g_loss, d_loss)\n",
    "\n",
    "print \"\\nDONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "plt.imshow(x_train[i:i+1, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d61d2bcd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d47a97950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d4294a090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.18774430e-08]\n",
      " [  1.17817081e-08]\n",
      " [  1.16360104e-08]]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 3\n",
    "\n",
    "gen_input = gi_sampler(num_samples, g_input_size)\n",
    "forgery = G.model.predict(gen_input)\n",
    "\n",
    "# print np.mean(forgery)\n",
    "\n",
    "\n",
    "for i in range(num_samples):\n",
    "\n",
    "    plt.imshow(forgery[i ,0])\n",
    "    plt.show()\n",
    "\n",
    "print D.model.predict(forgery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95926702,  0.93024564,  0.99963713,  0.99983162,  0.98766541,\n",
       "         0.82020974,  0.48736143,  0.20591736,  0.21359053,  0.32458568,\n",
       "         0.53126395,  0.94952166,  0.99957955,  0.99996644,  0.99996901,\n",
       "         0.27559862, -0.99999994, -1.        , -0.99999708,  0.23608355,\n",
       "         0.21488442,  0.20319615,  0.20258303,  0.35423985,  0.62418109,\n",
       "         0.96343535,  0.99965781,  0.88318092],\n",
       "       [ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999994,  0.99999994,  0.99999994,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        , -0.91377282, -0.99999708,  0.76959276,  0.99999994,\n",
       "         0.99999994,  0.99999994,  0.99999994,  0.99999994,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999994],\n",
       "       [ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999994,  0.99999994,  0.99999994,\n",
       "         0.99999994,  0.99999994,  1.        ,  1.        ,  1.        ,\n",
       "         0.99999952, -0.72654748, -0.99992985,  0.89353549,  1.        ,\n",
       "         0.99999994,  0.99999994,  0.99999994,  0.99999994,  0.99999994,\n",
       "         1.        ,  1.        ,  0.99999988],\n",
       "       [ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999964,  0.99999964,  0.9999997 ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999982],\n",
       "       [ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99862999,  0.99862719,  0.99985909,\n",
       "         0.99999976,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99999994,  0.99999994,\n",
       "         1.        ,  1.        ,  0.99997026],\n",
       "       [ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99935377,  0.99972636,  0.99996543,\n",
       "         0.99999994,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999982],\n",
       "       [ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999994,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99999696,  0.99999934,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99995917],\n",
       "       [ 0.9999997 ,  1.        ,  0.99999946,  0.99999309,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99998379,  0.99999964,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.87258852],\n",
       "       [ 0.99999988,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99986231],\n",
       "       [ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999708,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99997216],\n",
       "       [ 0.9999994 ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99999958,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999708,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999899],\n",
       "       [ 0.99999899,  0.99999988,  0.99999976,  0.99999982,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.9999969 ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99829477],\n",
       "       [ 0.99998075,  0.99992168,  0.99973011,  0.99989009,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.9999969 ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.78879893],\n",
       "       [ 0.99999708,  0.99999416,  0.99995548,  0.99986762,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99998915],\n",
       "       [ 0.99999982,  0.99999994,  0.99999881,  0.99995202,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99998915],\n",
       "       [ 0.99999946,  0.99999994,  0.9999997 ,  0.99999881,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99999988,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99996489],\n",
       "       [ 0.99999946,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99965209],\n",
       "       [ 0.99999946,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.97841054],\n",
       "       [ 0.99999946,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99999988,  1.        , -0.38599283],\n",
       "       [ 0.99999946,  1.        ,  0.99999994,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99999994,  1.        , -0.35312179],\n",
       "       [ 0.99999946,  1.        ,  0.99999732,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99388146],\n",
       "       [ 0.99999946,  0.99999994,  0.99997818,  0.99999982,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99993598],\n",
       "       [ 0.99999946,  0.99999988,  0.99998325,  0.99990374,  0.9999603 ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99999958,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99997872],\n",
       "       [ 0.99999946,  1.        ,  0.99999988,  0.9999997 ,  0.99982756,\n",
       "         0.99996305,  0.9999783 ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99998915],\n",
       "       [ 0.99999946,  0.99999988,  0.99999201,  0.99617171,  0.60369712,\n",
       "        -0.5832783 , -0.43193454,  0.99691266,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.99999863,\n",
       "         1.        ,  1.        ,  0.99999976,  0.99962074,  1.        ,\n",
       "         1.        ,  1.        ,  0.99998921],\n",
       "       [ 0.99999946,  0.99999994,  0.99999863,  0.99916506,  0.99917001,\n",
       "         0.99798906,  0.99956793,  0.99999917,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99999994,  0.99999905,\n",
       "         1.        ,  1.        ,  1.        ,  0.99999452,  1.        ,\n",
       "         1.        ,  1.        ,  0.99953175],\n",
       "       [ 0.9999994 ,  0.99999994,  0.99999851,  0.99917561,  0.99919367,\n",
       "         0.99993193,  0.9999997 ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99999994,  0.99999911,  0.99999279,\n",
       "         0.99999827,  0.99999982,  1.        ,  0.99981689,  0.99999148,\n",
       "         0.99999893,  0.99999928,  0.46792412],\n",
       "       [ 0.99972063,  0.99988085,  0.99926412,  0.87731898,  0.87980175,\n",
       "         0.98586518,  0.99992949,  0.99999928,  0.99999928,  0.99999946,\n",
       "         0.99999994,  1.        ,  0.99999893,  0.99993455,  0.80290997,\n",
       "        -0.64907563, -0.69943333, -0.72498393, -0.73990536, -0.73380554,\n",
       "        -0.66832888, -0.70479107,  0.97620511,  0.87937039,  0.89168221,\n",
       "         0.92162251,  0.94833255, -0.91540837]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forgery[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64774901,  0.47213832,  0.83213407,  0.20773771,  0.63707203,\n",
       "         0.23384756,  0.44012588,  0.38610521,  0.10582925,  0.59061027],\n",
       "       [ 0.5742842 ,  0.67586386,  0.62400943,  0.75729132,  0.49189976,\n",
       "         0.91653287,  0.82447982,  0.60402727,  0.58924794,  0.39663929],\n",
       "       [ 0.12640612,  0.53640133,  0.22328959,  0.60326886,  0.77125293,\n",
       "         0.19539379,  0.05754237,  0.97403169,  0.5050801 ,  0.44795224]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gi_sampler(num_samples, g_input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
