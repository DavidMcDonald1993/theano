{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal implementation of sparse coding layer\n",
    "\n",
    "source: https://github.com/EderSantana/blog/blob/master/2015-08-02%20sparse%20coding%20with%20keras.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GT 650M (0000:01:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import theano\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-83ffb9119459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSparseCoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, input_dim, output_dim,\n\u001b[1;32m      3\u001b[0m                  \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mtruncate_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Layer' is not defined"
     ]
    }
   ],
   "source": [
    "class SparseCoding(Layer):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 init='glorot_uniform',\n",
    "                 activation='linear',\n",
    "                 truncate_gradient=-1,\n",
    "                 gamma=.1, # \n",
    "                 n_steps=10,\n",
    "                 batch_size=100,\n",
    "                 return_reconstruction=False,\n",
    "                 W_regularizer=l2(.01),\n",
    "                 activity_regularizer=None):\n",
    "            \n",
    "            super(SparseCoding, self).__init__()\n",
    "            self.init = init\n",
    "            \n",
    "            self.A = self.init((self.output_dim, self.input_dim)) \n",
    "            # contrary to a regular neural net layer, here \n",
    "            # the output needs to have the same dimension as\n",
    "            # as the input we are modeling. Other layers would\n",
    "            # have self.init((self.input_dim, self.output_dim))\n",
    "            # as the dimensions of its adaptive coefficients.\n",
    "    \n",
    "    def get_output(self, train=False):\n",
    "        s = self.get_input(train) # input data to be modeled\n",
    "        initial_x = alloc_zeros_matrix(self.batch_size, self.output_dim) \n",
    "        # initialize sparse codes with zeros.\n",
    "        # Again note that the coefficients here got \n",
    "        # output_dim as its last dimension because this \n",
    "        # a generative model.\n",
    "        outputs, updates = theano.scan(\n",
    "                self._step, # function operated in the main loop\n",
    "                sequences=[], # iterable input sequences, we don't need this here\n",
    "                outputs_info=[initial_states, ]*3 + [None, ], # initial states, \n",
    "                # I'll explain why we have 4 initial states.\n",
    "                non_sequences=[inputs, prior], # this is kept the same for the entire for loop\n",
    "                n_steps=self.n_steps, # since sequences is empty, scan needs this \n",
    "                                      # information to know when to stop\n",
    "                truncate_gradient=self.truncate_gradient) # how much backpropagation \n",
    "                                                          # through time/iteration you need.\n",
    "        if self.return_reconstruction:\n",
    "                return outputs[-1][-1] # return the approximation of the input\n",
    "        else:\n",
    "                return outputs[0][-1] # return the sparse codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
