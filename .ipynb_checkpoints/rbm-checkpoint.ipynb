{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from keras.datasets import mnist\n",
    "import numpy\n",
    "\n",
    "class RBM(object):\n",
    "    \"\"\"Restricted Boltzmann Machine (RBM)  \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        hbias=None,\n",
    "        vbias=None,\n",
    "        numpy_rng=None,\n",
    "        theano_rng=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        RBM constructor. Defines the parameters of the model along with\n",
    "        basic operations for inferring hidden from visible (and vice-versa),\n",
    "        as well as for performing CD updates.\n",
    "\n",
    "        :param input: None for standalone RBMs or symbolic variable if RBM is\n",
    "        part of a larger graph.\n",
    "\n",
    "        :param n_visible: number of visible units\n",
    "\n",
    "        :param n_hidden: number of hidden units\n",
    "\n",
    "        :param W: None for standalone RBMs or symbolic variable pointing to a\n",
    "        shared weight matrix in case RBM is part of a DBN network; in a DBN,\n",
    "        the weights are shared between RBMs and layers of a MLP\n",
    "\n",
    "        :param hbias: None for standalone RBMs or symbolic variable pointing\n",
    "        to a shared hidden units bias vector in case RBM is part of a\n",
    "        different network\n",
    "\n",
    "        :param vbias: None for standalone RBMs or a symbolic variable\n",
    "        pointing to a shared visible units bias\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        if numpy_rng is None:\n",
    "            # create a number generator\n",
    "            numpy_rng = numpy.random.RandomState(1234)\n",
    "\n",
    "        if theano_rng is None:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        if W is None:\n",
    "            # W is initialized with `initial_W` which is uniformely\n",
    "            # sampled from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible)) the output of uniform if\n",
    "            # converted using asarray to dtype theano.config.floatX so\n",
    "            # that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            # theano shared variables for weights and biases\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        if hbias is None:\n",
    "            # create shared variable for hidden units bias\n",
    "            hbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='hbias',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if vbias is None:\n",
    "            # create shared variable for visible units bias\n",
    "            vbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='vbias',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        # initialize input layer for standalone RBM or layer0 of DBN\n",
    "        self.input = input\n",
    "        if not input:\n",
    "            self.input = T.matrix('input')\n",
    "\n",
    "        self.W = W\n",
    "        self.hbias = hbias\n",
    "        self.vbias = vbias\n",
    "        self.theano_rng = theano_rng\n",
    "        # **** WARNING: It is not a good idea to put things in this list\n",
    "        # other than shared variables created in this function.\n",
    "        self.params = [self.W, self.hbias, self.vbias]\n",
    "\n",
    "    def propup(self, vis):\n",
    "        '''This function propagates the visible units activation upwards to\n",
    "        the hidden units\n",
    "\n",
    "        Note that we return also the pre-sigmoid activation of the\n",
    "        layer. As it will turn out later, due to how Theano deals with\n",
    "        optimizations, this symbolic variable will be needed to write\n",
    "        down a more stable computational graph (see details in the\n",
    "        reconstruction cost function)\n",
    "\n",
    "        '''\n",
    "        pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    def sample_h_given_v(self, v0_sample):\n",
    "        ''' This function infers state of hidden units given visible units '''\n",
    "        # compute the activation of the hidden units given a sample of\n",
    "        # the visibles\n",
    "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
    "        # get a sample of the hiddens given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n",
    "                                             n=1, p=h1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    def propdown(self, hid):\n",
    "        '''This function propagates the hidden units activation downwards to\n",
    "        the visible units\n",
    "\n",
    "        Note that we return also the pre_sigmoid_activation of the\n",
    "        layer. As it will turn out later, due to how Theano deals with\n",
    "        optimizations, this symbolic variable will be needed to write\n",
    "        down a more stable computational graph (see details in the\n",
    "        reconstruction cost function)\n",
    "\n",
    "        '''\n",
    "        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    def sample_v_given_h(self, h0_sample):\n",
    "        ''' This function infers state of visible units given hidden units '''\n",
    "        # compute the activation of the visible given the hidden sample\n",
    "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
    "        # get a sample of the visible given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n",
    "                                             n=1, p=v1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    def gibbs_hvh(self, h0_sample):\n",
    "        ''' This function implements one step of Gibbs sampling,\n",
    "            starting from the hidden state'''\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample,\n",
    "                pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    def gibbs_vhv(self, v0_sample):\n",
    "        ''' This function implements one step of Gibbs sampling,\n",
    "            starting from the visible state'''\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample,\n",
    "                pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    def free_energy(self, v_sample):\n",
    "        ''' Function to compute the free energy '''\n",
    "        wx_b = T.dot(v_sample, self.W) + self.hbias\n",
    "        vbias_term = T.dot(v_sample, self.vbias)\n",
    "        hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n",
    "        return -hidden_term - vbias_term\n",
    "\n",
    "    def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n",
    "        \"\"\"This functions implements one step of CD-k or PCD-k\n",
    "\n",
    "        :param lr: learning rate used to train the RBM\n",
    "\n",
    "        :param persistent: None for CD. For PCD, shared variable\n",
    "            containing old state of Gibbs chain. This must be a shared\n",
    "            variable of size (batch size, number of hidden units).\n",
    "\n",
    "        :param k: number of Gibbs steps to do in CD-k/PCD-k\n",
    "\n",
    "        Returns a proxy for the cost and the updates dictionary. The\n",
    "        dictionary contains the update rules for weights and biases but\n",
    "        also an update of the shared variable used to store the persistent\n",
    "        chain, if one is used.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # compute positive phase\n",
    "        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
    "\n",
    "        # decide how to initialize persistent chain:\n",
    "        # for CD, we use the newly generate hidden sample\n",
    "        # for PCD, we initialize from the old state of the chain\n",
    "        if persistent is None:\n",
    "            chain_start = ph_sample\n",
    "        else:\n",
    "            chain_start = persistent\n",
    "\n",
    "\n",
    "        # perform actual negative phase\n",
    "        # in order to implement CD-k/PCD-k we need to scan over the\n",
    "        # function that implements one gibbs step k times.\n",
    "        # Read Theano tutorial on scan for more information :\n",
    "        # http://deeplearning.net/software/theano/library/scan.html\n",
    "        # the scan will return the entire Gibbs chain\n",
    "        (\n",
    "            [\n",
    "                pre_sigmoid_nvs,\n",
    "                nv_means,\n",
    "                nv_samples,\n",
    "                pre_sigmoid_nhs,\n",
    "                nh_means,\n",
    "                nh_samples\n",
    "            ],\n",
    "            updates\n",
    "        ) = theano.scan(\n",
    "            self.gibbs_hvh,\n",
    "            # the None are place holders, saying that\n",
    "            # chain_start is the initial state corresponding to the\n",
    "            # 6th output\n",
    "            outputs_info=[None, None, None, None, None, chain_start],\n",
    "            n_steps=k,\n",
    "            name=\"gibbs_hvh\"\n",
    "        )\n",
    "\n",
    "        # determine gradients on RBM parameters\n",
    "        # note that we only need the sample at the end of the chain\n",
    "        chain_end = nv_samples[-1]\n",
    "\n",
    "        cost = T.mean(self.free_energy(self.input)) - T.mean(\n",
    "            self.free_energy(chain_end))\n",
    "        # We must not compute the gradient through the gibbs sampling\n",
    "        gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n",
    "\n",
    "        # constructs the update dictionary\n",
    "        for gparam, param in zip(gparams, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - gparam * T.cast(\n",
    "                lr,\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "        if persistent:\n",
    "            # Note that this works only if persistent is a shared variable\n",
    "            updates[persistent] = nh_samples[-1]\n",
    "            # pseudo-likelihood is a better proxy for PCD\n",
    "            monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n",
    "        else:\n",
    "            # reconstruction cross-entropy is a better proxy for CD\n",
    "            monitoring_cost = self.get_reconstruction_cost(updates,\n",
    "                                                           pre_sigmoid_nvs[-1])\n",
    "\n",
    "        return monitoring_cost, updates\n",
    "        \n",
    "    def get_pseudo_likelihood_cost(self, updates):\n",
    "        \"\"\"Stochastic approximation to the pseudo-likelihood\"\"\"\n",
    "\n",
    "        # index of bit i in expression p(x_i | x_{\\i})\n",
    "        bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n",
    "\n",
    "        # binarize the input image by rounding to nearest integer\n",
    "        xi = T.round(self.input)\n",
    "\n",
    "        # calculate free energy for the given bit configuration\n",
    "        fe_xi = self.free_energy(xi)\n",
    "\n",
    "        # flip bit x_i of matrix xi and preserve all other bits x_{\\i}\n",
    "        # Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns\n",
    "        # the result to xi_flip, instead of working in place on xi.\n",
    "        xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n",
    "\n",
    "        # calculate free energy with bit flipped\n",
    "        fe_xi_flip = self.free_energy(xi_flip)\n",
    "\n",
    "        # equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\n",
    "        cost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip -\n",
    "                                                            fe_xi)))\n",
    "\n",
    "        # increment bit_i_idx % number as part of updates\n",
    "        updates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n",
    "\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'RandomStreams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0d5770f8a7a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRBM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-71e38c61b5f0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input, n_visible, n_hidden, W, hbias, vbias, numpy_rng, theano_rng)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtheano_rng\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mtheano_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomStreams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_rng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mW\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'RandomStreams' is not defined"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "rbm = RBM(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-eb8eaf9489c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# the purpose of train_rbm is solely to update the RBM parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m train_rbm = theano.function(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "    # it is ok for a theano function to have no output\n",
    "    # the purpose of train_rbm is solely to update the RBM parameters\n",
    "    train_rbm = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        },\n",
    "        name='train_rbm'\n",
    "    )\n",
    "\n",
    "    plotting_time = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # go through the training set\n",
    "        mean_cost = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            mean_cost += [train_rbm(batch_index)]\n",
    "\n",
    "        print('Training epoch %d, cost is ' % epoch, numpy.mean(mean_cost))\n",
    "\n",
    "        # Plot filters after each training epoch\n",
    "        plotting_start = timeit.default_timer()\n",
    "        # Construct image from the weight matrix\n",
    "        image = Image.fromarray(\n",
    "            tile_raster_images(\n",
    "                X=rbm.W.get_value(borrow=True).T,\n",
    "                img_shape=(28, 28),\n",
    "                tile_shape=(10, 10),\n",
    "                tile_spacing=(1, 1)\n",
    "            )\n",
    "        )\n",
    "        image.save('filters_at_epoch_%i.png' % epoch)\n",
    "        plotting_stop = timeit.default_timer()\n",
    "        plotting_time += (plotting_stop - plotting_start)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    pretraining_time = (end_time - start_time) - plotting_time\n",
    "\n",
    "    print ('Training took %f minutes' % (pretraining_time / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #################################\n",
    "    #     Sampling from the RBM     #\n",
    "    #################################\n",
    "    # find out the number of test samples\n",
    "    number_of_test_samples = test_set_x.get_value(borrow=True).shape[0]\n",
    "\n",
    "    # pick random test examples, with which to initialize the persistent chain\n",
    "    test_idx = rng.randint(number_of_test_samples - n_chains)\n",
    "    persistent_vis_chain = theano.shared(\n",
    "        numpy.asarray(\n",
    "            test_set_x.get_value(borrow=True)[test_idx:test_idx + n_chains],\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    plot_every = 1000\n",
    "    # define one step of Gibbs sampling (mf = mean-field) define a\n",
    "    # function that does `plot_every` steps before returning the\n",
    "    # sample for plotting\n",
    "    (\n",
    "        [\n",
    "            presig_hids,\n",
    "            hid_mfs,\n",
    "            hid_samples,\n",
    "            presig_vis,\n",
    "            vis_mfs,\n",
    "            vis_samples\n",
    "        ],\n",
    "        updates\n",
    "    ) = theano.scan(\n",
    "        rbm.gibbs_vhv,\n",
    "        outputs_info=[None, None, None, None, None, persistent_vis_chain],\n",
    "        n_steps=plot_every,\n",
    "        name=\"gibbs_vhv\"\n",
    "    )\n",
    "\n",
    "    # add to updates the shared variable that takes care of our persistent\n",
    "    # chain :.\n",
    "    updates.update({persistent_vis_chain: vis_samples[-1]})\n",
    "    # construct the function that implements our persistent chain.\n",
    "    # we generate the \"mean field\" activations for plotting and the actual\n",
    "    # samples for reinitializing the state of our persistent chain\n",
    "    sample_fn = theano.function(\n",
    "        [],\n",
    "        [\n",
    "            vis_mfs[-1],\n",
    "            vis_samples[-1]\n",
    "        ],\n",
    "        updates=updates,\n",
    "        name='sample_fn'\n",
    "    )\n",
    "\n",
    "    # create a space to store the image for plotting ( we need to leave\n",
    "    # room for the tile_spacing as well)\n",
    "    image_data = numpy.zeros(\n",
    "        (29 * n_samples + 1, 29 * n_chains - 1),\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    for idx in range(n_samples):\n",
    "        # generate `plot_every` intermediate samples that we discard,\n",
    "        # because successive samples in the chain are too correlated\n",
    "        vis_mf, vis_sample = sample_fn()\n",
    "        print(' ... plotting sample %d' % idx)\n",
    "        image_data[29 * idx:29 * idx + 28, :] = tile_raster_images(\n",
    "            X=vis_mf,\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(1, n_chains),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "\n",
    "    # construct image\n",
    "    image = Image.fromarray(image_data)\n",
    "    image.save('samples.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
